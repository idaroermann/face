{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint,shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=open(\"dictionary.txt\", 'r')\n",
    "s=open(\"sentiment_labels.txt\")\n",
    "sents=[]\n",
    "labels=[]\n",
    "for line in d:\n",
    "    sents.append(line.split('|')[0])\n",
    "for line in s:\n",
    "    score=line.split('|')[1]\n",
    "    labels.append(score.split('\\n')[0])\n",
    "d.close()\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hot_vector(size, sent_feat_temps):\n",
    "    sent_feats = np.zeros(size)\n",
    "    for element in sent_feat_temps:\n",
    "        sent_feats[element] = 1    \n",
    "    return sent_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab(fts_list):#takes a list of lists of words\n",
    "    rV=[]\n",
    "    for el in fts_list:\n",
    "        for word in el:\n",
    "            rV.extend(el)\n",
    "    rV=set(rV)\n",
    "    return rV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_sents=[]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for sent in sents:\n",
    "    sentence=word_tokenize(sent)\n",
    "    lem_sentence=[]\n",
    "    for word in sentence:\n",
    "        lem_sentence.append(lemmatizer.lemmatize(word.lower()))\n",
    "    token_sents.append(lem_sentence)\n",
    "#    print(sentence)\n",
    "#    input()\n",
    "zipped=list(zip(token_sents,labels))\n",
    "del zipped[0]#getting rid of the header\n",
    "zipped.sort(key = lambda x: len(x[0]), reverse=True)\n",
    "language_data=zipped[:100000]\n",
    "#list of tuples. One per sentence. ([list of lemmatized lowercase words in the sentence], a sentiment score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentlist=[sent for sent,score in language_data]\n",
    "vocab=create_vocab(sentlist)\n",
    "vocabsize=len(vocab)\n",
    "inttofeat = dict(zip(range(vocabsize), vocab))\n",
    "feattoint = dict(zip(vocab, range(vocabsize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hot_data=[]\n",
    "for sent,score in language_data:\n",
    "    feat=[feattoint[word] for word in sent]\n",
    "    hot_data.append((hot_vector(vocabsize,feat),score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now, sort the sentences in 3 categories:\n",
    "\n",
    "angry_sents=[]\n",
    "happy_sents=[]\n",
    "neutral_sents=[]\n",
    "\n",
    "for sent,score in hot_data:\n",
    "    if 0.0 <= float(score) <= 0.25:\n",
    "        angry_sents.append((sent,score))\n",
    "    elif float(score) == 0.50:\n",
    "        neutral_sents.append((sent,score))\n",
    "    elif 0.75 <= float(score) <= 1:\n",
    "        happy_sents.append((sent,score))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"fer2013/fer2013.csv\")#image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
    "angry_df=df[df.emotion == 0]\n",
    "happy_df=df[df.emotion == 3]\n",
    "neutral_df=df[df.emotion == 6]\n",
    "#len(angry_sents)#  7926\n",
    "#len(neutral_sents)# around 19000\n",
    "#len(happy_sents) # 12094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "for sent,score in angry_sents:\n",
    "    image_feat=angry_df.iloc[randint(0, len(angry_df)-1)]['pixels'].split()\n",
    "    image_feat = [int(i) for i in image_feat]\n",
    "    all_data.append((np.array(image_feat),sent,'NEG'))\n",
    "    \n",
    "for sent,score in happy_sents:\n",
    "    image_feat=happy_df.iloc[randint(0, len(happy_df)-1)]['pixels'].split()\n",
    "    image_feat = [int(i) for i in image_feat]\n",
    "    all_data.append((np.array(image_feat),sent,'POS'))\n",
    "\n",
    "for sent,score in neutral_sents:\n",
    "    image_feat=neutral_df.iloc[randint(0, len(neutral_df)-1)]['pixels'].split()\n",
    "    image_feat = [int(i) for i in image_feat]\n",
    "    all_data.append((np.array(image_feat),sent,'NEU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s=int((len(all_data)*0.8))#splits with 80% training, 20% test\n",
    "train, test=all_data[:s], all_data[s:]\n",
    "\n",
    "#small_data=all_data[:5000]\n",
    "#s=int((len(small_data)*0.8))#splits with 80% training, 20% test\n",
    "#strain, stest=small_data[:s], small_data[s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain=[(image,sent) for (image,sent,label) in train]\n",
    "ytrain=[label for (image,sent,label) in train]\n",
    "xtest=[(image,sent) for (image,sent,label) in test]\n",
    "ytest=[label for (image,sent,label) in test]\n",
    "\n",
    "#xtrain=[(image,sent) for (image,sent,label) in strain]\n",
    "#ytrain=[label for (image,sent,label) in strain]\n",
    "#xtest=[(image,sent) for (image,sent,label) in stest]\n",
    "#ytest=[label for (image,sent,label) in stest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size:  2304\n",
      "sent_size:  18049\n",
      "labels:   {'POS', 'NEU', 'NEG'}\n"
     ]
    }
   ],
   "source": [
    "image_size=len(all_data[0][0])\n",
    "sent_size=len(all_data[0][1])\n",
    "labels=set(list(l for (i,s,l) in all_data))\n",
    "\n",
    "#image_size=len(small_data[0][0])\n",
    "#sent_size=len(small_data[0][1])\n",
    "#labels=set(list(l for (i,s,l) in small_data))\n",
    "\n",
    "print('image_size: ',image_size)\n",
    "print('sent_size: ',sent_size)\n",
    "print('labels:  ', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xtrain is now a list of tuples, but xtrain in Keras must be a list of two numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain2 = list(zip(*xtrain))\n",
    "xtrain2 = [np.array(xtrain2[1]), np.array(xtrain2[0])]\n",
    "\n",
    "xtest2 = list(zip(*xtest))\n",
    "xtest2 = [np.array(xtest2[1]), np.array(xtest2[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#since we are using ‘sparse_categorical_crossentropy’ ,\n",
    "#it means the output prediction must be categorical and each category is represented with an integer\n",
    "cat_codes = {\n",
    "    'POS': 0,\n",
    "    'NEU': 1,\n",
    "    'NEG': 2,\n",
    "}\n",
    "ytrain2 = np.array([cat_codes[c] for c in ytrain])\n",
    "ytest2 = np.array([cat_codes[c] for c in ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image feat type:  <class 'numpy.ndarray'>\n",
      "sent feat type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('image feat type: ', type(all_data[0][0]))\n",
    "print('sent feat type:', type(all_data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Concatenate, Dropout\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaption of Mehdi’s code for Visual QA:\n",
    "\n",
    "input question is the sentence --- input image is image --- output answer is the sentiment label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 18049)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 18049, 50)     902450      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            20200       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 2304)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2304)          117504      lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 4608)          0           input_2[0][0]                    \n",
      "                                                                   dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2304)          10619136    concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 2304)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 3)             6915        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 11,666,205\n",
      "Trainable params: 11,666,205\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_question = Input([question_len+1,])\n",
    "#input_context = Input([visual_vec_len,])\n",
    "\n",
    "input_question = Input([sent_size,])\n",
    "input_context = Input([image_size,])\n",
    "\n",
    "# learn embedings (size=50 as we chose just now :D)\n",
    "q_embs = Embedding(len(vocab), 50)(input_question)\n",
    "\n",
    "# encode the question\n",
    "q_encoded = LSTM(50)(q_embs)\n",
    "\n",
    "mlp_1 = Dense(image_size, activation='tanh')(q_encoded)\n",
    "\n",
    "q_composed = Concatenate()([input_context, mlp_1])\n",
    "\n",
    "mlp_2 = Dropout(0.2)(Dense(image_size, activation='relu')(q_composed))\n",
    "\n",
    "final_a = Dense(len(labels), activation='softmax')(mlp_2)\n",
    "\n",
    "model = Model([input_question, input_context], final_a)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile('adam', 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(xtrain2, ytrain2, epochs=100, batch_size=32, validation_split=0.1, callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('saved_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict([xtest[0][:1], xtest[1][:1]])\n",
    "#print('answer predictions', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facial expressions dataset: \n",
    "\"Challenges in Representation Learning: A report on three machine learning\n",
    "contests.\" I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B\n",
    "Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,\n",
    "X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,\n",
    "M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and\n",
    "Y. Bengio. arXiv 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
