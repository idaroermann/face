{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial/handson session is designed based on the Visual Turing Test tutorial in here:\n",
    "https://github.com/mateuszmalinowski/visual_turing_test-tutorial\n",
    "Visual Turing Challenge\n",
    "Mateusz Malinowski and Mario Fritz\n",
    "Max-Plank Institute\n",
    "\n",
    "Mehdi Ghanimifard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "import numpy as np\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image as kimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Concatenate, Dropout\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is to show how Mehdi's code from the tutorial would work. In my case, my image model predicts classes (angry/happy), and not features. My bottleneck_fc_model.h5 predicts features, but these are actually already loaded in bottleneck_features_train and validation.npy (from run of eslp_image.py), which I load in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads my image_model (the one that predicts features, not classes) \n",
    "\n",
    "#pretrained_cnn_model = load_model('2bottleneck_fc_model.h5')\n",
    "#pretrained_cnn_model = load_model('2bottleneck_fc_model.h5')\n",
    "\n",
    "# a function (from image file path to feature vectors)\n",
    "# From Mehdi's tutorial\n",
    "\n",
    "#def img2vec(image_path):\n",
    "#    x = kimage.load_img(image_path, target_size=[48,48])#from 224,224\n",
    "#    x_array = kimage.img_to_array(x)\n",
    "#    xs_array = np.array([x_array,])\n",
    "#    # notice that we are not using full capacity of the GPU when we are passing only one image per prediction.\n",
    "#    # we could have a larger batch.\n",
    "#    return pretrained_cnn_model.predict(preprocess_input(xs_array)).flatten()\n",
    "\n",
    "#use model to extract features from .png format pictures with our function img2vec: \n",
    "\n",
    "#angry_images=[]\n",
    "#happy_images=[]\n",
    "\n",
    "#path = \"pngs/happy/output*.png\"\n",
    "#for png in glob.glob(path):\n",
    "#    happy_images.append(img2vec(png))\n",
    "#print('Happy faces:   ', len(happy_images))  \n",
    "\n",
    "#path = \"pngs/angry/output*.png\"\n",
    "#for png in glob.glob(path):\n",
    "#    angry_images.append(img2vec(png))\n",
    "#print('Angry faces:   ', len(angry_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_train=np.load('bottleneck_features_train.npy')\n",
    "b_val=np.load('bottleneck_features_validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7904, 1, 1, 512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1952, 1, 1, 512)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these features are half angry, half happy, so I save them accordingly:\n",
    "angry_images_train=b_train[:int((7904/2))]\n",
    "angry_images_test=b_val[:int((1952/2))]\n",
    "happy_images_train=b_train[int((7904/2)):]\n",
    "happy_images_test=b_val[int((1952/2)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load sentences (from run of preprocessing.py): \n",
    "happy_sents=np.load('happy_sents.npy')#With sentiment score above 0.6\n",
    "angry_sents=np.load('angry_sents.npy')#With sentiment score below 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splits each category and input in train and test before mapping. \n",
    "#When mapping, one image is paired with some sentences, and we dont want to shuffle some anwers into the test set.\n",
    "\n",
    "#train_angry_images=angry_images[:int(0.8*len(angry_images))]\n",
    "#test_angry_images=angry_images[int(0.8*len(angry_images)):]\n",
    "\n",
    "#train_happy_images=happy_images[:int(0.8*len(happy_images))]\n",
    "#test_happy_images=happy_images[int(0.8*len(happy_images)):]\n",
    "\n",
    "train_angry_sents=angry_sents[:int(0.8*len(angry_sents))]\n",
    "test_angry_sents=angry_sents[int(0.8*len(angry_sents)):]\n",
    "\n",
    "train_happy_sents=happy_sents[:int(0.8*len(happy_sents))]\n",
    "test_happy_sents=happy_sents[int(0.8*len(happy_sents)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save(\"angry_images.npy\", angry_images)\n",
    "#np.save(\"happy_images.npy\", happy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#angry_images = [\n",
    "#    img2vec(\"pngs/angry/{0}.png\".format(image_name.strip()))\n",
    "#    for image_name in open('pngs/angry/{0}.png')\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code takes time:\n",
    "#X_images = [\n",
    "#    img2vec(\"data/daquar/images/{0}.png\".format(image_name.strip()))\n",
    "#    for image_name in open('data/daquar/qa.894.raw.train.format_triple.contexts')\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(angry_images_train[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pairs:  57184\n"
     ]
    }
   ],
   "source": [
    "#Maps each image with with the chosen sentences. \n",
    "#Every image is mapped with more than one sentence, to grow the data size\n",
    "\n",
    "#Angry face with negative sentences\n",
    "#happy face with positive sentences\n",
    "#Saves the sentiment category as 'NEG' or 'POS'\n",
    "\n",
    "\n",
    "train=[]\n",
    "test=[]\n",
    "\n",
    "sent_count=0\n",
    "for image in angry_images_train:    \n",
    "    for step in range(6):\n",
    "        train.append((image[0][0], train_angry_sents[sent_count][0], 'NEG'))\n",
    "        sent_count=+1\n",
    "sent_count=0\n",
    "for image in happy_images_train:    \n",
    "    for step in range(6):\n",
    "        train.append((image[0][0], train_happy_sents[sent_count][0], 'POS'))\n",
    "        sent_count=+1\n",
    "        \n",
    "sent_count=0        \n",
    "for image in happy_images_test:\n",
    "    for step in range(5):\n",
    "        test.append((image[0][0], test_happy_sents[sent_count][0], 'POS'))\n",
    "        sent_count=+1 \n",
    "sent_count=0        \n",
    "\n",
    "for image in angry_images_test:\n",
    "    for step in range(5):\n",
    "        test.append((image[0][0], test_angry_sents[sent_count][0], 'NEG'))\n",
    "        sent_count=+1 \n",
    "    \n",
    "print('total pairs: ',len(train)+len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we can shuffle in each set:\n",
    "shuffle(train)\n",
    "shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s=int((len(all_data)*0.8))#splits with 80% training, 20% test\n",
    "#train, test=all_data[:s], all_data[s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain=[(image,sent) for (image,sent,label) in train]\n",
    "ytrain=[label for (image,sent,label) in train]\n",
    "xtest=[(image,sent) for (image,sent,label) in test]\n",
    "ytest=[label for (image,sent,label) in test]\n",
    "\n",
    "#image_size=len(train[0][0])\n",
    "#sent_size=len(train[0][1])\n",
    "#labels=set(list(l for (i,s,l) in all_data))\n",
    "\n",
    "xtrain2 = list(zip(*xtrain))\n",
    "xtrain2 = [np.array(xtrain2[1]), np.array(xtrain2[0])]\n",
    "\n",
    "xtest2 = list(zip(*xtest))\n",
    "xtest2 = [np.array(xtest2[1]), np.array(xtest2[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEG', 'POS'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(l for l in ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_codes = {\n",
    "    'POS': 0,\n",
    "    'NEG': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain2 = np.array([cat_codes[c] for c in ytrain])\n",
    "ytest2 = np.array([cat_codes[c] for c in ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"xtrain0.npy\", xtrain2[0])#image\n",
    "np.save(\"xtrain1.npy\", xtrain2[1])#sent\n",
    "np.save(\"ytrain.npy\", ytrain2)#label\n",
    "np.save(\"xtest0.npy\", xtest2[0])#image\n",
    "np.save(\"xtest1.npy\", xtest2[1])#sent\n",
    "np.save(\"ytest.npy\", ytest2)#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size=len(xtrain[0][0])\n",
    "sent_size=len(xtrain[0][1])\n",
    "vocab = list(np.load('vocab.npy'))\n",
    "labels = {'POS', 'NEG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#xtrain = [np.load('xtrain0.npy'), np.load('xtrain1.npy')]\n",
    "#ytrain = np.load('ytrain.npy')\n",
    "#xtest = [np.load('xtest0.npy'), np.load('xtest1.npy')]\n",
    "#ytest = np.load('ytest.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 56, 50)       905600      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 50)           20200       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          26112       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           input_8[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          524800      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2)            1026        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,477,738\n",
      "Trainable params: 1,477,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_question = Input([sent_size,])\n",
    "input_context = Input([image_size,])\n",
    "\n",
    "# learn embedings (size=50 as we chose just now :D)\n",
    "q_embs = Embedding(len(vocab), 50)(input_question)\n",
    "\n",
    "# encode the question\n",
    "q_encoded = LSTM(50)(q_embs)\n",
    "\n",
    "mlp_1 = Dense(image_size, activation='tanh')(q_encoded)\n",
    "\n",
    "q_composed = Concatenate()([input_context, mlp_1])\n",
    "\n",
    "mlp_2 = Dropout(0.2)(Dense(image_size, activation='relu')(q_composed))\n",
    "#mlp_2 = Dropout(0.2)(Dense(image_size, activation='relu')(mlp_1))\n",
    "\n",
    "final_a = Dense(len(labels), activation='softmax')(mlp_2)\n",
    "\n",
    "model = Model([input_question, input_context], final_a)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile('adam', 'sparse_categorical_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history=model.fit(xtrain2, ytrain2, epochs=50, batch_size=64, validation_split=0.1, callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('combi_model.h5')  # creates a HDF5 file 'my_model.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history)\n",
    "modelfile = open('./combihistory.txt',\"w\")\n",
    "modelfile.write(str(history.history))\n",
    "modelfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('combiloss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('combiaccuracy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.evaluate(xtest2, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions = model.predict([xtest[0][:1], xtest[1][:1]])\n",
    "#print('answer predictions', predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
