{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial/handson session is designed based on the Visual Turing Test tutorial in here:\n",
    "https://github.com/mateuszmalinowski/visual_turing_test-tutorial\n",
    "Visual Turing Challenge\n",
    "Mateusz Malinowski and Mario Fritz\n",
    "Max-Plank Institute\n",
    "\n",
    "Mehdi Ghanimifard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "import numpy as np\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image as kimage\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Embedding, Concatenate, Dropout\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is to show how Mehdi's code from the tutorial would work. In my case, my image model predicts classes (angry/happy), and not features. My bottleneck_fc_model.h5 predicts features, but these are actually already loaded in bottleneck_features_train and validation.npy (from run of eslp_image.py), which I load in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads my image_model (the one that predicts features, not classes) \n",
    "\n",
    "#pretrained_cnn_model = load_model('2bottleneck_fc_model.h5')\n",
    "#pretrained_cnn_model = load_model('2bottleneck_fc_model.h5')\n",
    "\n",
    "# a function (from image file path to feature vectors)\n",
    "# From Mehdi's tutorial\n",
    "\n",
    "def img2vec(image_path):\n",
    "    x = kimage.load_img(image_path, target_size=[48,48])#from 224,224\n",
    "    x_array = kimage.img_to_array(x)\n",
    "    xs_array = np.array([x_array,])\n",
    "    # notice that we are not using full capacity of the GPU when we are passing only one image per prediction.\n",
    "    # we could have a larger batch.\n",
    "    return pretrained_cnn_model.predict(preprocess_input(xs_array)).flatten()\n",
    "\n",
    "#use model to extract features from .png format pictures with our function img2vec: \n",
    "\n",
    "#angry_images=[]\n",
    "#happy_images=[]\n",
    "\n",
    "#path = \"pngs/happy/output*.png\"\n",
    "#for png in glob.glob(path):\n",
    "#    happy_images.append(img2vec(png))\n",
    "#print('Happy faces:   ', len(happy_images))  \n",
    "\n",
    "#path = \"pngs/angry/output*.png\"\n",
    "#for png in glob.glob(path):\n",
    "#    angry_images.append(img2vec(png))\n",
    "#print('Angry faces:   ', len(angry_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#angry_images = [\n",
    "#    img2vec(\"pngs/angry/{0}.png\".format(image_name.strip()))\n",
    "#    for image_name in open('pngs/angry/{0}.png')\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save(\"angry_images.npy\", angry_images)\n",
    "#np.save(\"happy_images.npy\", happy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_train=np.load('bottleneck_features_train.npy')\n",
    "b_val=np.load('bottleneck_features_validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7904, 1, 1, 512)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1952, 1, 1, 512)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these features are half angry, half happy, so I save them accordingly:\n",
    "angry_images_train=b_train[:int((7904/2))]\n",
    "angry_images_test=b_val[:int((1952/2))]\n",
    "happy_images_train=b_train[int((7904/2)):]\n",
    "happy_images_test=b_val[int((1952/2)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, 1, 1, 512)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angry_images_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load sentences (from run of language_preprocessing.py): \n",
    "\n",
    "happy_sents=np.load('happy_sents.npy')#With sentiment score above 0.6\n",
    "angry_sents=np.load('angry_sents.npy')#With sentiment score below 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splits each category and input in train and test before mapping. \n",
    "#When mapping, one image is paired with some sentences, and we dont want to shuffle some anwers into the test set.\n",
    "\n",
    "#train_angry_images=angry_images[:int(0.8*len(angry_images))]\n",
    "#test_angry_images=angry_images[int(0.8*len(angry_images)):]\n",
    "\n",
    "#train_happy_images=happy_images[:int(0.8*len(happy_images))]\n",
    "#test_happy_images=happy_images[int(0.8*len(happy_images)):]\n",
    "\n",
    "train_angry_sents=angry_sents[:int(0.8*len(angry_sents))]\n",
    "test_angry_sents=angry_sents[int(0.8*len(angry_sents)):]\n",
    "\n",
    "train_happy_sents=happy_sents[:int(0.8*len(happy_sents))]\n",
    "test_happy_sents=happy_sents[int(0.8*len(happy_sents)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pairs:  57184\n"
     ]
    }
   ],
   "source": [
    "#Maps each image with with the chosen sentences. \n",
    "#Every image is mapped with more than one sentence, to grow the data size\n",
    "\n",
    "#Angry face with negative sentences\n",
    "#happy face with positive sentences\n",
    "#Saves the sentiment category as 'NEG' or 'POS'\n",
    "\n",
    "\n",
    "train=[]\n",
    "test=[]\n",
    "\n",
    "sent_count=0\n",
    "for image in angry_images_train:    \n",
    "    for step in range(6):\n",
    "        train.append((image[0][0], train_angry_sents[sent_count][0], 'NEG'))\n",
    "        sent_count=+1\n",
    "sent_count=0\n",
    "for image in happy_images_train:    \n",
    "    for step in range(6):\n",
    "        train.append((image[0][0], train_happy_sents[sent_count][0], 'POS'))\n",
    "        sent_count=+1\n",
    "        \n",
    "sent_count=0        \n",
    "for image in happy_images_test:\n",
    "    for step in range(5):\n",
    "        test.append((image[0][0], test_happy_sents[sent_count][0], 'POS'))\n",
    "        sent_count=+1 \n",
    "sent_count=0        \n",
    "\n",
    "for image in angry_images_test:\n",
    "    for step in range(5):\n",
    "        test.append((image[0][0], test_angry_sents[sent_count][0], 'NEG'))\n",
    "        sent_count=+1 \n",
    "    \n",
    "print('total pairs: ',len(train)+len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we can shuffle in each set:\n",
    "shuffle(train)\n",
    "shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain=[(image,sent) for (image,sent,label) in train]\n",
    "ytrain=[label for (image,sent,label) in train]\n",
    "xtest=[(image,sent) for (image,sent,label) in test]\n",
    "ytest=[label for (image,sent,label) in test]\n",
    "\n",
    "#image_size=len(train[0][0])\n",
    "#sent_size=len(train[0][1])\n",
    "#labels=set(list(l for (i,s,l) in all_data))\n",
    "\n",
    "xtrain2 = list(zip(*xtrain))\n",
    "xtrain2 = [np.array(xtrain2[1]), np.array(xtrain2[0])]\n",
    "\n",
    "xtest2 = list(zip(*xtest))\n",
    "xtest2 = [np.array(xtest2[1]), np.array(xtest2[0])]#[sents, images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_codes = {\n",
    "    'POS': 1,\n",
    "    'NEG': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain2 = np.array([cat_codes[c] for c in ytrain])\n",
    "ytest2 = np.array([cat_codes[c] for c in ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"xtrain0.npy\", xtrain2[0])#sents\n",
    "np.save(\"xtrain1.npy\", xtrain2[1])#images\n",
    "np.save(\"ytrain.npy\", ytrain2)#label\n",
    "np.save(\"xtest0.npy\", xtest2[0])#\n",
    "np.save(\"xtest1.npy\", xtest2[1])#\n",
    "np.save(\"ytest.npy\", ytest2)#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size=len(xtrain2[1][0])\n",
    "sent_size=len(xtrain2[0][0])\n",
    "vocab = list(np.load('vocab.npy'))\n",
    "labels = {'POS', 'NEG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#xtrain = [np.load('xtrain0.npy'), np.load('xtrain1.npy')]\n",
    "#ytrain = np.load('ytrain.npy')\n",
    "#xtest = [np.load('xtest0.npy'), np.load('xtest1.npy')]\n",
    "#ytest = np.load('ytest.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  3413,  5532,  8426],\n",
       "       [    0,     0,     0, ...,  3413,  5532,  8426],\n",
       "       [    0,     0,     0, ...,  8373, 17064, 14872],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  3413,  5532,  8426],\n",
       "       [    0,     0,     0, ...,  8373, 17064, 14872],\n",
       "       [    0,     0,     0, ...,  3413,  5532,  8426]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.9062693 , ..., 0.4142326 , 0.5021698 ,\n",
       "        0.00232075],\n",
       "       [0.82107186, 0.        , 1.0568473 , ..., 0.        , 0.27432036,\n",
       "        0.        ],\n",
       "       [0.908764  , 0.        , 0.        , ..., 0.        , 0.96987593,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.74720246, 0.        , 0.        , ..., 0.80306214, 0.3244789 ,\n",
       "        0.        ],\n",
       "       [0.9740577 , 0.        , 1.325321  , ..., 0.        , 0.        ,\n",
       "        0.1857352 ],\n",
       "       [0.37611964, 0.        , 0.        , ..., 0.2590153 , 0.12551779,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 56, 50)       905600      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 50)           20200       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          26112       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1024)         0           input_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          524800      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            1026        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,477,738\n",
      "Trainable params: 1,477,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_question = Input([sent_size,])\n",
    "input_context = Input([image_size,])\n",
    "\n",
    "# learn embedings (size=50 as we chose just now :D)\n",
    "q_embs = Embedding(len(vocab), 50)(input_question)\n",
    "\n",
    "# encode the question\n",
    "q_encoded = LSTM(50)(q_embs)\n",
    "\n",
    "mlp_1 = Dense(image_size, activation='tanh')(q_encoded)\n",
    "\n",
    "q_composed = Concatenate()([input_context, mlp_1])\n",
    "\n",
    "mlp_2 = Dropout(0.2)(Dense(image_size, activation='relu')(q_composed))\n",
    "#mlp_2 = Dropout(0.2)(Dense(image_size, activation='relu')(mlp_1))\n",
    "\n",
    "final_a = Dense(len(labels), activation='softmax')(mlp_2)\n",
    "\n",
    "model = Model([input_question, input_context], final_a)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile('adam', 'sparse_categorical_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history=model.fit(xtrain2, ytrain2, epochs=50, batch_size=64, validation_split=0.1, callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('combi_model.h5')  # creates a HDF5 file 'my_model.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [1.1920928244535389e-07, 1.1920928244535389e-07, 1.1920928244535389e-07, 1.1920928244535389e-07, 1.1920928244535389e-07, 1.1920928244535389e-07], 'val_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'loss': [0.06765512260154777, 1.1920928243636347e-07, 1.1920928243636347e-07, 1.1920928243636347e-07, 1.1920928243636347e-07, 1.1920928243636347e-07], 'acc': [0.9666807254323071, 1.0, 1.0, 1.0, 1.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)\n",
    "modelfile = open('./combihistory.txt',\"w\")\n",
    "modelfile.write(str(history.history))\n",
    "modelfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('combiloss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('combiaccuracy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952/1952 [==============================] - 2s 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.059047769327632, 0.5]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest2, ytest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(xtest2, ytest2, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
